{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from itertools import compress\n",
    "from itertools import chain\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "data_path = os.path.join(\"D:/\", \"data\", \"drmkc\")\n",
    "pdf_path = os.path.join(data_path, \"pdfs\")\n",
    "\n",
    "filename = \"drr-scrape_total_20210505.json\"\n",
    "\n",
    "with open(os.path.join(data_path, filename), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "domain_url = \"https://\" + urlparse(data[0].get('url')).netloc\n",
    "pdfs = list(set([urljoin(domain_url, url) for url in list(chain(*[list(compress(entry['links'], [(\".pdf\" in link) for link in entry['links']])) for entry in data]))]))\n",
    "\n",
    "if not os.path.isdir(pdf_path):\n",
    "    os.mkdir(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data:\n",
    "    entry['domain_url'] = urlparse(entry.get('url')).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_kw = ['social media', 'government', 'participatory', 'youtube', 'nongovernmental organizations', 'notifications']\n",
    "\n",
    "data_exclude = [entry for entry in data if not all([kw in exclude_kw for kw in entry.get('keywords_matched')])]\n",
    "\n",
    "#data_exclude[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_links_manual = ['twitter', 'facebook', 'youtube', 'google', 'nature.com', 'zoom.us', 'goo.gl', 'bit.ly', 'flcikr', 't.co', 'medium.com', 'github', 'tandfonline', 'linkedin', 'bbc.co.uk', 'news24.com', 'vimeo', 'dx.doi.org']\n",
    "\n",
    "data_exclude = [entry for entry in data if not any([url in entry.get('url') for url in drop_links_manual])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_urls = list(set([entry.get('domain_url') for entry in data_exclude]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drmkc.jrc.ec.europa.eu',\n",
       " 'covid-statistics.jrc.ec.europa.eu',\n",
       " 'www.who.int',\n",
       " 'eur-lex.europa.eu',\n",
       " 'startnetwork.org',\n",
       " 'www.securityresearch-cou.eu',\n",
       " 'www.undrr.org',\n",
       " 'dppa.un.org']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_urls = [url for url in domain_urls if url != 'www.who.int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading pdfs for drmkc.jrc.ec.europa.eu\n",
      "\n",
      "|==================================================| 100.00 %\n",
      "\n",
      "downloading pdfs for covid-statistics.jrc.ec.europa.eu\n",
      "\n",
      "\n",
      "\n",
      "downloading pdfs for eur-lex.europa.eu\n",
      "\n",
      "\n",
      "\n",
      "downloading pdfs for startnetwork.org\n",
      "\n",
      "|==================================================| 100.00 %\n",
      "\n",
      "downloading pdfs for www.securityresearch-cou.eu\n",
      "\n",
      "\n",
      "\n",
      "downloading pdfs for www.undrr.org\n",
      "\n",
      "\n",
      "\n",
      "downloading pdfs for dppa.un.org\n",
      "\n",
      "|==================================================| 100.00 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def url_to_filename(url):\n",
    "    url = re.sub(r'(https\\:\\/\\/(www\\.)?)|(http\\:\\/\\/(www\\.)?)', '', url)\n",
    "    url = re.sub(r'\\:\\d{2,4}(?=\\/)', '', url)\n",
    "    urlpart = re.search(r'(\\w+?)\\.\\w{2,11}(\\.\\w{2,5})?(?=\\/)', url).group(1)\n",
    "    namepart = re.search(r'\\.\\w{2,11}(\\/.+\\.pdf)', url).group(1).replace(\"/\", \"-\").replace(\"\\\\\", \"-\")\n",
    "    namepart = namepart.replace(\"?\", \"\")\n",
    "    filename = urlpart + namepart\n",
    "    return(filename)\n",
    "\n",
    "for domain_url in domain_urls:\n",
    "    \n",
    "    missed_pdfs = []\n",
    "    \n",
    "    save_path = os.path.join(pdf_path, domain_url)\n",
    "    \n",
    "    if not os.path.isdir(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    \n",
    "    domain_set = [entry for entry in data_exclude if entry.get('domain_url') == domain_url]\n",
    "    \n",
    "    url_prefix = \"https://\" + domain_url\n",
    "    \n",
    "    pdfs = list(set([urljoin(url_prefix, url) for url in list(chain(*[list(compress(entry['links'], [(\".pdf\" in link) for link in entry['links']])) for entry in domain_set]))]))\n",
    "    \n",
    "    print(\"downloading pdfs for {}\\n\".format(domain_url))\n",
    "    for c, pdf_url in enumerate(pdfs, start = 1):\n",
    "    \n",
    "        filename = url_to_filename(pdf_url)\n",
    "        \n",
    "        if os.path.isfile(os.path.join(save_path, filename)):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(pdf_url, stream=True)\n",
    "        except:\n",
    "            missed_pdfs.append(pdf_url)\n",
    "            continue\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            time.sleep(random.uniform(0.5, 1))\n",
    "        else:\n",
    "            missed_pdfs.append(pdf_url)\n",
    "            continue\n",
    "\n",
    "        progress = \"|{0}| {1:.2f} %\".format((\"=\"*int(c/len(pdfs) * 50)).ljust(50), c/len(pdfs) * 100)\n",
    "    \n",
    "        print(progress, end = \"\\r\")\n",
    "        \n",
    "        with open(os.path.join(save_path, 'missed_pdf.txt'), 'w', encoding = 'utf-8') as f:\n",
    "            for url in missed_pdfs:\n",
    "                f.write(url + \"\\n\")\n",
    "            f.close()\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://who.foundation/cms/wp-content/uploads/2020/11/TFIU_HEALING_ARTS_PR_6-10-2020_WHO-1.pdf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = re.sub(r'(https\\:\\/\\/(www\\.)?)|(http\\:\\/\\/(www\\.)?)', '', pdf_url)\n",
    "url = re.sub(r'\\:\\d{2,4}(?=\\/)', '', url)\n",
    "urlpart = re.search(r'(\\w+?)\\.\\w{2,5}(\\.\\w{2,5})?(?=\\/|\\:)', url).group(1)\n",
    "namepart = re.search(r'\\.\\w{2,5}(\\/.+\\.pdf)', url).group(1).replace(\"/\", \"-\").replace(\"\\\\\", \"-\")\n",
    "filename = urlpart + namepart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ggtc-dmdocuments-5.3%20toolkit%202015.pdf'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imperial-bitstream-10044-1-77482-14-2020-03-16-COVID19-Report-9.pdf'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
