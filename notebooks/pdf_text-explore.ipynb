{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import re\n",
    "import pdfplumber\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "data_path = os.path.join(\"D:/\", \"data\", \"drmkc\", \"pdfs\")\n",
    "work_path = os.path.join(\"D:/\", \"data\", \"drmkc\", \"work\")\n",
    "filenames = [join(data_path,f) for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "filenames = [filename for filename in filenames if filename.endswith('.pdf')]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "nlp.max_length = 2500000\n",
    "\n",
    "mallet_path = os.path.join('C:\\\\', 'mallet', 'mallet-2.0.8', 'bin', 'mallet.bat') # update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|==================================================| 100.00 %\r"
     ]
    }
   ],
   "source": [
    "problem_files = []\n",
    "\n",
    "for c, filename in enumerate(filenames, start = 1):\n",
    "    \n",
    "    if filename in [entry.get('filename') for entry in data]:\n",
    "        continue\n",
    "    \n",
    "    progress = \"|{0}| {1:.2f} %\".format((\"=\"*int(c/len(filenames) * 50)).ljust(50), c/len(filenames) * 100)\n",
    "    \n",
    "    entry = {}\n",
    "    entry['filename'] = filename\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(join(data_path, filename)) as pdf:\n",
    "            try:\n",
    "                pdf_text = '\\n'.join([page.extract_text() for page in pdf.pages if page.extract_text() is not None])\n",
    "                entry['text'] = pdf_text\n",
    "            except Exception as e:\n",
    "                print(filename)\n",
    "                raise e\n",
    "    except:\n",
    "        problem_files.append(filename)\n",
    "    \n",
    "    data.append(entry)\n",
    "    \n",
    "    print(progress, end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [entry for entry in data if 'text' in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "stop_words = list(nlp.Defaults.stop_words)\n",
    "\n",
    "def tokenizer_custom(text, stop_words=stop_words, tags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    doc = nlp(text)\n",
    "        \n",
    "    pos_tags = tags\n",
    "    \n",
    "    tokens = []\n",
    "      \n",
    "    for word in doc:\n",
    "        if (len(word.text) < 2):\n",
    "            continue\n",
    "        if word.pos_ in pos_tags:\n",
    "            token = word.text.lower() # Returning the word in lower-case.\n",
    "            tokens.append(token)\n",
    "    \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|==================================================| 100.00 %\r"
     ]
    }
   ],
   "source": [
    "for c, entry in enumerate(data, start = 1):\n",
    "    entry['tokens'] = tokenizer_custom(entry.get('text'))\n",
    "    \n",
    "    progress = \"|{0}| {1:.2f} %\".format((\"=\"*int(c/len(data) * 50)).ljust(50), c/len(data) * 100)\n",
    "    print(progress, end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "outname = \"drmkc-pdfs_tokenized.json\"\n",
    "\n",
    "with open(os.path.join(work_path, outname), 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [entry.get('tokens') for entry in data]\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(tokens_list) # integer id per word\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(tokens) for tokens in tokens_list] # bag-of-word(bow) tuple for each text - id, count\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)\n",
    "lda_model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "pprint(lda_model.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score - https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=comments_tokens, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamodel = coherence_model_lda.get_coherence() \n",
    "print('\\nCoherence Score: ', coherence_ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting dominant topic for one corpus entry (bag-of-word tuple - bowt)\n",
    "def get_dominant_topic(corpus_bowt, ldamodel = lda_model):\n",
    "    \n",
    "    dominant_topic_dict = dict()\n",
    "    \n",
    "    topics_doc = ldamodel[corpus_bowt]\n",
    "    \n",
    "    dominant_topic = sorted(topics_doc, key = lambda t: t[1], reverse = True)[0]\n",
    "    topic_num = dominant_topic[0]\n",
    "    topic_prob = dominant_topic[1]\n",
    "    \n",
    "    topic_kws = [word for word, prop in ldamodel.show_topic(topic_num)]\n",
    "    \n",
    "    dominant_topic_dict['dominant_topic'] = topic_num\n",
    "    dominant_topic_dict['topic_probability'] = topic_prob\n",
    "    dominant_topic_dict['topic_keywords'] = topic_kws\n",
    "    \n",
    "    return(dominant_topic_dict) # Note that domninant topic info is returned as dictionary\n",
    "\n",
    "# Creating list of dictionaries - one dictionary contatining dominant topic info for each corpus entry\n",
    "corpus_dominant_topics = list()\n",
    "for bowt in corpus:\n",
    "    dominant_topic = get_dominant_topic(bowt)\n",
    "    corpus_dominant_topics.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow**\n",
    "\n",
    "- [x] Read in pdfs\n",
    "- [x] tokenize\n",
    "- [ ] filter keywords\n",
    "- [ ] keyword analysis\n",
    "- [ ] topic model (see reddit-miner example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
